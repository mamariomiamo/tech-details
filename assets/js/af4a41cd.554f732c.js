(self.webpackChunktech_details=self.webpackChunktech_details||[]).push([[7604],{3905:function(e,t,n){"use strict";n.d(t,{Zo:function(){return c},kt:function(){return u}});var a=n(67294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,r=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),m=p(n),u=i,f=m["".concat(s,".").concat(u)]||m[u]||d[u]||r;return n?a.createElement(f,o(o({ref:t},c),{},{components:n})):a.createElement(f,o({ref:t},c))}));function u(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=n.length,o=new Array(r);o[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:i,o[1]=l;for(var p=2;p<r;p++)o[p]=n[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},61963:function(e,t,n){"use strict";n.r(t),n.d(t,{frontMatter:function(){return l},contentTitle:function(){return s},metadata:function(){return p},toc:function(){return c},default:function(){return m}});var a=n(22122),i=n(19756),r=(n(67294),n(3905)),o=["components"],l={hide_title:!0,sidebar_label:"Basalt Frontend (Optical Flow)"},s="Basalt Front End",p={unversionedId:"research/vio/basalt-frontend",id:"research/vio/basalt-frontend",isDocsHomePage:!1,title:"Basalt Front End",description:"The front end include two types of optical flow: frame to frame optical flow and patch optical frame.",source:"@site/docs/research/vio/basalt-frontend.md",sourceDirName:"research/vio",slug:"/research/vio/basalt-frontend",permalink:"/tech-details/docs/research/vio/basalt-frontend",version:"current",lastUpdatedAt:1628956952,formattedLastUpdatedAt:"8/14/2021",frontMatter:{hide_title:!0,sidebar_label:"Basalt Frontend (Optical Flow)"},sidebar:"researchSidebar",previous:{title:"Basalt Backend (VioEstimator)",permalink:"/tech-details/docs/research/vio/basalt-backend"},next:{title:"System Level Tests",permalink:"/tech-details/docs/research/vio/basalt-tests"}},c=[{value:"Initialization",id:"initialization",children:[{value:"Parameters",id:"parameters",children:[]}]},{value:"data structure",id:"data-structure",children:[]},{value:"FrameToFrameOpticalFlow",id:"frametoframeopticalflow",children:[{value:"Constructor",id:"constructor",children:[]},{value:"processingLoop",id:"processingloop",children:[]},{value:"processFrame",id:"processframe",children:[]}]}],d={toc:c};function m(e){var t=e.components,l=(0,i.Z)(e,o);return(0,r.kt)("wrapper",(0,a.Z)({},d,l,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"basalt-front-end"},"Basalt Front End"),(0,r.kt)("p",null,"The front end include two types of optical flow: frame to frame optical flow and patch optical frame. "),(0,r.kt)("h2",{id:"initialization"},"Initialization"),(0,r.kt)("p",null,"The front end is initialized in "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},"basalt::OpticalFlowFactory::getOpticalFlow(vio_config, calib)\n")),(0,r.kt)("p",null,"func param: vio_config: parameters in config.json file; calib: camera info. "),(0,r.kt)("p",null,"return: ",(0,r.kt)("strong",{parentName:"p"},"OpticalFlowBase::Ptr"),", which is reset (initialized) by ",(0,r.kt)("em",{parentName:"p"},"config.optical_flow_type")," and ",(0,r.kt)("em",{parentName:"p"},"config.optical_flow_pattern")," parameter shown in the table."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},"class OpticalFlowBase {\n public:\n  using Ptr = std::shared_ptr<OpticalFlowBase>;\n  tbb::concurrent_bounded_queue<OpticalFlowInput::Ptr> input_queue;\n  tbb::concurrent_bounded_queue<OpticalFlowResult::Ptr>* output_queue = nullptr;\n  Eigen::MatrixXf patch_coord;\n};\n")),(0,r.kt)("p",null,"The ",(0,r.kt)("strong",{parentName:"p"},"input_queue")," is set to be the stereo image buffer queue with tbb::concurrent_bounded_queue to handle multithread sharing.\nThe ",(0,r.kt)("strong",{parentName:"p"},"output_queue")," passes to vio vision_data_queue."),(0,r.kt)("h3",{id:"parameters"},"Parameters"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"param"),(0,r.kt)("th",{parentName:"tr",align:null},"type"),(0,r.kt)("th",{parentName:"tr",align:null},"value used"),(0,r.kt)("th",{parentName:"tr",align:null},"function"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"config.optical_flow_type"),(0,r.kt)("td",{parentName:"tr",align:null},'"frame_to_frame" / "patch"'),(0,r.kt)("td",{parentName:"tr",align:null},'"frame_to_frame"'),(0,r.kt)("td",{parentName:"tr",align:null},'decide optical flow method: "patch" to use PatchOpticalFlow and "frame_to_frame" to use FrameToFrameOpticalFlow')),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"config.optical_flow_pattern"),(0,r.kt)("td",{parentName:"tr",align:null},"24/52/51/50"),(0,r.kt)("td",{parentName:"tr",align:null},"50"),(0,r.kt)("td",{parentName:"tr",align:null},"optical flow pattern shown in the below image")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null})))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},"Optical flow patterns:\nThe pattern is used for each key point to buid a patch when comparing matches\npattern24:\n           00  01\n  \n       02  03  04  05\n  \n   06  07  08  09  10  11\n  \n   12  13  14  15  16  17\n  \n       18  19  20  21\n  \n           22  23\n\npattern52:\n           00  01  02  03\n  \n       04  05  06  07  08  09\n  \n   10  11  12  13  14  15  16  17\n  \n   18  19  20  21  22  23  24  25\n  \n   26  27  28  29  30  31  32  33\n  \n   34  35  36  37  38  39  40  41\n  \n       42  43  44  45  46  47\n  \n           48  49  50  51 \n\nWeights of pattern52:\n                       {-3, 7},  {-1, 7},  {1, 7},   {3, 7},\n                {-5, 5},  {-3, 5},  {-1, 5},  {1, 5},   {3, 5},  {5, 5},\n      {-7, 3},  {-5, 3},  {-3, 3},  {-1, 3},  {1, 3},  {3, 3}, {5, 3},   {7, 3},\n      {-7, 1},  {-5, 1},  {-3, 1},  {-1, 1},  {1, 1},  {3, 1}, {5, 1},   {7, 1},\n      {-7, -1}, {-5, -1}, {-3, -1}, {-1, -1}, {1, -1}, {3, -1}, {5, -1},  {7, -1},\n      {-7, -3}, {-5, -3}, {-3, -3}, {-1, -3}, {1, -3}, {3, -3}, {5, -3},  {7, -3},\n               {-5, -5}, {-3, -5}, {-1, -5}, {1, -5},  {3, -5}, {5, -5},\n                      {-3, -7}, {-1, -7}, {1, -7},  {3, -7}\n\npattern51 is the same as Pattern52 but twice smaller in terms of weight.\npattern50 is the same as Pattern52 but 0.75 smaller.\n\n")),(0,r.kt)("h2",{id:"data-structure"},"data structure"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},"struct OpticalFlowInput {\n  using Ptr = std::shared_ptr<OpticalFlowInput>;\n  int64_t t_ns;\n  std::vector<ImageData> img_data;\n};\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},"struct OpticalFlowResult {\n  using Ptr = std::shared_ptr<OpticalFlowResult>;\n  int64_t t_ns;\n  //observation size depends on the number of cameras, 2 in our case\n  //KeypointId:\n  //AffineCompact2f:\n  std::vector<Eigen::aligned_map<KeypointId, Eigen::AffineCompact2f>> observations;  \n  OpticalFlowInput::Ptr input_images; // one image frame from the input_queue of images\n};\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},"/// ids for 2D features detected in images\nusing FeatureId = int;\n/// identifies a frame of multiple images (stereo pair)\nusing FrameId = int64_t;\n/// identifies the camera (left or right)\nusing CamId = std::size_t;\n\nstruct KeypointsData {\n  /// collection of 2d corner points: pixel location  (indexed by FeatureId) \n  std::vector<Eigen::Vector2d, Eigen::aligned_allocator<Eigen::Vector2d>> corners;\n  /// collection of feature orientation (in radian) with same index as `corners`(indexed by FeatureId)\n  std::vector<double> corner_angles;\n  /// collection of feature descriptors with same index as `corners` (indexed by FeatureId)\n  std::vector<std::bitset<256>> corner_descriptors;\n  Eigen::aligned_vector<Eigen::Vector4d> corners_3d;\n  std::vector<FeatureHash> hashes;\n  HashBowVector bow_vector;\n};\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},"Patch\n")),(0,r.kt)("h2",{id:"frametoframeopticalflow"},"FrameToFrameOpticalFlow"),(0,r.kt)("h3",{id:"constructor"},"Constructor"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"calculate R and T between if there is more than one cameras."),(0,r.kt)("li",{parentName:"ul"},"initialize the ",(0,r.kt)("inlineCode",{parentName:"li"},"processingLoop()")," thread and use a smart pointer to handle resource.")),(0,r.kt)("h3",{id:"processingloop"},"processingLoop"),(0,r.kt)("p",null,"Keep poping OpticalFlowInput from the input_queue and process it with ",(0,r.kt)("inlineCode",{parentName:"p"},"processFrame(input_ptr->t_ns, input_ptr)")," function until we get a nullptr from the input_queue."),(0,r.kt)("h3",{id:"processframe"},"processFrame"),(0,r.kt)("p",null,"Input(int64_t curr_t_ns, OpticalFlowInput::Ptr& new_img_vec)"),(0,r.kt)("h4",{id:"first-frame-initialization"},"first frame initialization"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"initialize ",(0,r.kt)("strong",{parentName:"li"},"transforms")," (OpticalFlowResult::Ptr)")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},"transforms.reset(new OpticalFlowResult);\ntransforms->input_images = new_img_vec; \n")),(0,r.kt)("ol",{start:2},(0,r.kt)("li",{parentName:"ol"},"initialize ",(0,r.kt)("strong",{parentName:"li"},"pyramid"),"(std::shared_ptr<std::vector<basalt::ManagedImagePyr<u_int16_t>>>)")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},"pyramid->resize(calib.intrinsics.size());\n")),(0,r.kt)("ol",{start:3},(0,r.kt)("li",{parentName:"ol"},"running pyramid for all images in parallel")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},'tbb::parallel_for(tbb::blocked_range<size_t>(0, calib.intrinsics.size()),\n                        [&](const tbb::blocked_range<size_t>& r) {\n                          for (size_t i = r.begin(); i != r.end(); ++i) {\n                            pyramid->at(i).setFromImage(\n                                *new_img_vec->img_data[i].img,\n                                config.optical_flow_levels);\n                          }\n                        });\n\n//[tbb.parallel_for(blocked_range<T>(begin,end,grainsize), func(const tbb::blocked_range<size_t>& r))](https://software.intel.com/en-us/node/506057)\n//[&] means "capture by reference", [=] means "capture by value"\n//setFromImage(const ManagedImage<T>& other, size_t num_levels) sets image pyramid from other image\n// @param other image to use for the pyramid level 0\n')),(0,r.kt)("p",null,"The ",(0,r.kt)("strong",{parentName:"p"},"ManagedImagePyr")," class is used to calculate pyramid of the input image. Noted that the pyramid is stored as a (1+1/2)w x h image shown as the zebra image. A gaussian kernel is used to subsampling the image and every even-numbered row and colum have to be removed. The kernel is shown as this template."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},"1/256 * cov[image x kernel] + 0.5\n1 & 4 & 6 & 4 & 1\n4 & 16 & 24 & 16 & 4\n6 & 24 & 36 & 24 & 6\n4 & 16 & 24 & 16 & 4\n1 & 4 & 6 & 4 & 1\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},'// only left important function here\ntemplate <typename T, class Allocator = DefaultImageAllocator<T>>\nclass ManagedImagePyr {\n public:\n  using Ptr = std::shared_ptr<ManagedImagePyr<T, Allocator>>;\n\n  /// @brief Set image pyramid from other image.\n  inline void setFromImage(const ManagedImage<T>& other, size_t num_levels) {\n    // call "Image<const T> lvl(size_t lvl)" \n        to build subimage on the image, i.e. given x, y index as well as w and h for each pyramid\n    // call "void subsample(const Image<const T>& img, Image<T>& img_sub)" \n                to return const image of the certain level, one by convolution with Gaussian kernel(ubsample the image twice in vertical and horizontal direction) and removing every even-numbered row and column  \n    }\n\n protected:\n  size_t orig_w;          ///< Width of the original image (level 0)\n  ManagedImage<T> image;  ///< Pyramid   // (1+1/2)w x h \n};\n')),(0,r.kt)("p",null,"The paramid 0,1...n are stored from left to right and upper to bottom as shown in the image."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"pyramid",src:n(68370).Z})),(0,r.kt)("ol",{start:4},(0,r.kt)("li",{parentName:"ol"},"addPoints")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"add previously points from the Left image to pts0(vector<Eigen::Vector2d>) variable"),(0,r.kt)("li",{parentName:"ul"},"detect points at level 0 ")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},"void detectKeypoints(\n    const basalt::Image<const uint16_t>& img_raw, KeypointsData& kd,\n    int PATCH_SIZE, int num_points_cell,\n    const Eigen::aligned_vector<Eigen::Vector2d>& current_points){\n    devide image into cells based on PATCH_SIZE\n    put existing points into each cell\n    for each cell:\n        if the current cell already has previously tracked feature, skip this cell\n        else detect fast feature in the grid (default: one feature per grid): cv::FAST(subImg, points, threshold); threshold is from 40->20->10->5 and make sure the point is with in EDGE_THRESHOLD = 19 of the whole image\n    }\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"put corners with keypoint_id in ",(0,r.kt)("strong",{parentName:"li"},"transform"),"(local variable) and initialize new_poses0(same type as transform: Eigen::AffineCompact2f). note: transfrom is just the keypoint location on image"),(0,r.kt)("li",{parentName:"ul"},"track points from left to right  ")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},'void trackPoints(const basalt::ManagedImagePyr<u_int16_t>& pyr_1,\n                   const basalt::ManagedImagePyr<u_int16_t>& pyr_2,\n                   const Eigen::aligned_map<KeypointId, Eigen::AffineCompact2f>&\n                       transform_map_1,\n                   Eigen::aligned_map<KeypointId, Eigen::AffineCompact2f>&\n                       transform_map_2, bool leftToRight = false){\n/*\n    1. trackPointAtLevel: Check one pyr level: if the residul = patch - data, is valid\n          valid means that 1.1 at least PATTERN_SIZE / 2 number of points that both valid in template and residuals patch\n                           1.2 the points transformed by the updated and optimized T can still be seen in the image \n      2. trackPoint: check if the traking is valid at all pyr levels\n      3. This Func: square norm diff from "left to right transform" and "right to left transform" should be less than config.optical_flow_max_recovered_dist2\n\n*/}\n')),(0,r.kt)("ol",{start:5},(0,r.kt)("li",{parentName:"ol"},"filterPoints")),(0,r.kt)("h4",{id:"supporting-functions"},"supporting functions"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"trackPoints")),(0,r.kt)("p",null,"Track pointss between two images pyramid. Images could be from left to right/ right to left and previous to current. Noted that parameter without const here means output of the function. Also pay attention to the name, transform_map and transforms are map and vector, while transform is a single keypoint from the containers. It also applies to functions such as trackpoints and trackpoint."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},"  void trackPoints(const basalt::ManagedImagePyr<u_int16_t>& pyr_1,\n                   const basalt::ManagedImagePyr<u_int16_t>& pyr_2,\n                   const Eigen::aligned_map<KeypointId, Eigen::AffineCompact2f>&\n                       transform_map_1,\n                   Eigen::aligned_map<KeypointId, Eigen::AffineCompact2f>&\n                       transform_map_2) const {\n\nfor each point in transform_map_1:\n    if trackPoint(pyr_1, pyr_2, transform_1, transform_2) is valid (means more than half of the points in template and tracking patch at each leavel are within image boundries during optimization iterations)\n        track points from pyr_2 to pyr_1 (valid = trackPoint(pyr_2, pyr_1, transform_2, transform_1_recovered))\n            if the norm between the transform_1 and transform_1_recovered are within in config.optical_flow_max_recovered_dist2, then add this point into the transform_map_2\n    \n\n}\n")),(0,r.kt)("ol",{start:2},(0,r.kt)("li",{parentName:"ol"},"trackpoint\nTrack point from coarse to fine at all levels. The point is tracked from old_pyramid to current pyramid.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},"inline bool trackPoint(const basalt::ManagedImagePyr<uint16_t>& old_pyr,\n                         const basalt::ManagedImagePyr<uint16_t>& pyr,\n                         const Eigen::AffineCompact2f& old_transform,\n                         Eigen::AffineCompact2f& transform)\n                         {\n                            for each level of the pyramid(start from the highest one):\n                                scale the keypoint with respect to the transform (keypoint coordinate at the lowest layer of the pyramid)\n                                build a template patch around that scaled keypoint (see the patch section to see more details)\n                                perform tracking on current level (trackPointAtLevel(pyr.lvl(level), p, transform)) and check if the result is valid (check the definition in trackPointAtLevel func). \n                            update transform\n                            return patch_valid (True when all levels are valid)\n\n\n                         }\n")),(0,r.kt)("ol",{start:3},(0,r.kt)("li",{parentName:"ol"},"trackPointAtLevel\nTrack one point at one pyramid level. Return patch_valid, which means ")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},"inline bool trackPointAtLevel(const Image<const u_int16_t>& img_2,\n                                const PatchT& dp,\n                                Eigen::AffineCompact2f& transform) const {\n                                  for patch_valid && iteration < config.optical_flow_max_iterations:\n                                    transform designed pattern with transform matrix(R * (x;y) + t, its linear/rotation part is initialized as an identity matrix, and translation part is initialized as the keypoint's coordinate)\n                                    calculate residual between the template(dp) and the transformed patch(transformed_pat)\n                                      check out patch section for more details in terms of residual calculation\n                                    if the residual is valid:\n                                      calculate increment of the rotation: inc= inv(JT*J)*JT*res\n                                      update transfrom with inc \n                                      if transform.translation() is within the img_2, then the patch is still valid, otherwise break the loop with patch_valid = false\n\n                                    else:\n                                     patch_valid = false; break the loop\n\n\n\n\n                                }\n")),(0,r.kt)("ol",{start:4},(0,r.kt)("li",{parentName:"ol"},"OpticalFlowPatch structure\nThis structure is used to buid a patch around a point in image, and provides function to calculate residual used for transform itertation. Noted that below are pseudo code only.")),(0,r.kt)("p",null,"Using the inverse compositional algorithm discribed in section 3.2.1 in ",(0,r.kt)("a",{parentName:"p",href:"http://www.ncorr.com/download/publications/bakerunify.pdf"},"Lucas-Kanade 20 Years On: A Unifying Framework")," "),(0,r.kt)("p",null,"The original problem is to minimize the sum of squard error between two images, the template $$T$$ and the image $$I$$ warped back on to the coordinate frame of the template:"),(0,r.kt)("p",null,"  $$\\sum_x","[I(W(x;p))-T(x)]","^2$$"),(0,r.kt)("p",null,"  We could assume that a current estimate of p is known and then iteratively solves for increments to the parameters $$\\Delta p$$, i.e. the following expression is minimized: "),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Forwards additive(Lucas-Kanade) algorithm: $$\\sum_x","[I(W(x;p+\\Delta p))-T(x)]","^2$$ or ")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Frowards compositional algorithm: $$\\sum_x","[I(W(W(x;\\Delta p);p))-T(x)]","^2$$ "))),(0,r.kt)("p",null,"However, in order to avoid a huge computational cost in re-evaluating the Hessian in every iteration, we switch the role of the image and the template to form the inverse problem, where the goal is to minimize:"),(0,r.kt)("p",null,"  $$\\sum_x","[T(W(x;\\Delta p)-I(W(x;p)))]","^2$$"),(0,r.kt)("p",null,"  with respect to $$\\Delta p$$ and then update the warp:"),(0,r.kt)("p",null,"  $$W(x;p) \\leftarrow W(x;p) \\circ W(x;\\Delta p)^{-1}$$"),(0,r.kt)("p",null,"Performing a first order Taylor expansion gives:\n$$\\sum_x","[T(W(x;0))+ \\triangledown T \\frac{\\partial W}{\\partial p} - I(W(x;p))]","^2$$"),(0,r.kt)("p",null,"The solution to this least-squares problem is:\n$$\\Delta p = H^{-1} \\sum_x","[\\triangledown T\\frac{\\partial W}{\\partial p} ]","^T","[I(W(x;p))-T(x)]","$$"),(0,r.kt)("p",null,"where H is:\n$$H = \\sum_x ","[\\triangledown T\\frac{\\partial W}{\\partial p} ]","^T ","[\\triangledown T\\frac{\\partial W}{\\partial p} ]","$$"),(0,r.kt)("p",null,"Noted that the Jacobian $$\\frac{\\partial W}{\\partial p}$$ is evaluated at (x;0) and since there is nothing in the Hssian that depends on p, it is constant across iterations and can be pre-computed. The detail diagram of the algorithm is as follow."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"inverse compositional Algorithm",src:n(83002).Z})),(0,r.kt)("p",null,"In Baslt, W is group SE(2),which is rigid transformations in 2D space. It has 3 degree of freedom: $$p = ","[t_x, t_y, \\theta]","$$. The derivative of SE(2) is a 2x3 matrix $$Jw _ se2 = \\triangledown T\\frac{\\partial W}{\\partial p}$$ , where $$\\triangledown T$$ means template gradient and $$\\frac{\\partial W}{\\partial p}$$ means the change of x or y with respect to SE2 parameters $$","[t_x, t_y, \\theta]","$$. For details can check out Equ. 117 in ",(0,r.kt)("a",{parentName:"p",href:"http://www.ethaneade.org/lie.pdf"},"SE2"),".\nSE2 can be represented by:\n$$SE2=\\begin{bmatrix}\n0 & -\\theta & t_x","\\","\n\\theta & 0 & t_y","\\","\n0 & 0 & 0\n\\end{bmatrix}$$"),(0,r.kt)("p",null,"$$\\bm{x} = ","[x, y, 1]","^T$$"),(0,r.kt)("p",null,"And $$W(x;p) = SE(2) \\cdot \\bm{x} = ","[- \\theta y + t_x; \\theta x + t_y; 0]","$$. Thus we have:"),(0,r.kt)("p",null,"$$\\frac{\\partial W}{\\partial p}=\\begin{bmatrix}\n1 & 0 & -y","\\","\n0 & 1 & x\n\\end{bmatrix}$$"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},"\n\nstruct OpticalFlowPatch {\n  // build a patch around the center point pos in the given image\n  OpticalFlowPatch(const Image<const uint16_t> &img, const Vector2 &pos) {\n    setFromImage(img, pos); \n  }\n  void setFromImage(const Image<const uint16_t> &img, const Vector2 &pos) {\n    MatrixP2 grad; // matrix of patter_size * 2, stores the gradient, with respect to x (right) and y (down), for data!\n    for i in range(pattern)\n      Vector2 p = pos + pattern2.col(i);\n      if (img.InBounds(p, 2)) {\n        // see below images of the interGrad to know what it means\n        // valGrad[0] is the interplated pixel value, valGrad[1] is grad in x direction, valGrad[2] is grad in y dirextion\n        Vector3 valGrad = img.interpGrad<Scalar>(p);\n        obtain intensity value for each point in the patch: data[i] = valGrad[0];\n        update sum for with each intensity value: sum += valGrad[0];\n        calculate each row of grad and update grad sum from the last two data of valGrad\n        num_valid_points++;\n      } else {\n        data[i] = -1;\n      }\n    mean = sum / num_valid_points;\n    Scalar mean_inv = num_valid_points / sum;\n    // Yu: SE2: http://www.ethaneade.org/gradlie.pdf, see Equ. 119\n    // http://www.ncorr.com/download/publications/bakerunify.pdf\n    Eigen::Matrix<Scalar, 2, 3> Jw_se2; // hm: each column, x, y, theta. x points right, y points down\n    Jw_se2.template topLeftCorner<2, 2>().setIdentity();\n\n    MatrixP3 J_se2; // hm: pattern_size * 3, account for 2 translation and 1 rotation. reflects how each residual component changes, with regards to SE(2)\n\n    for (int i = 0; i < PATTERN_SIZE; i++) {\n      if (data[i] >= 0) {\n        const Scalar data_i = data[i];\n        const Vector2 grad_i = grad.row(i);\n        // hm: grad_i is the current pixel's gradient\n        // hm: effectively = grad_i / averaged_intensity -  averaged_grad * data_i / averaged_intensity\n        ///Yu: \n        grad.row(i) =\n            num_valid_points * (grad_i * sum - grad_sum * data_i) / (sum * sum);\n\n        data[i] *= mean_inv; // hm: divide by mean for all pixels\n      } else {\n        grad.row(i).setZero();\n      }\n\n      // Fill jacobians with respect to SE2 warp\n      // hm: the image is store in a row-major fashion, starting from the top-left corner. x correspon\n      /// Yu: SE2 param [x,y,theta]\n      Jw_se2(0, 2) = -pattern2(1, i); // hm: change of x, respect to rotation. Apply small angle differentiation, give dx/dtheta = -y\n      Jw_se2(1, 2) = pattern2(0, i); // hm: change of y, respect to rotation. Similarly dy/dtheta = x\n      // hm: J_se2 = di/dw * dw/dse2 = grad * Jw_se2\n      J_se2.row(i) = grad.row(i) * Jw_se2; // hm: jecobian of each pixel = \n    }\n\n    // hm: formulation refer to Usenko(2019), Preliminaries, with W = identity\n    Matrix3 H_se2 = J_se2.transpose() * J_se2;\n    Matrix3 H_se2_inv;\n    H_se2_inv.setIdentity();\n    H_se2.ldlt().solveInPlace(H_se2_inv); // hm: this is to solve the inverse. This is because H_se2_inv is set to identity. so the result x IS THE INVERSE\n\n    H_se2_inv_J_se2_T = H_se2_inv * J_se2.transpose();\n  }\n\n  inline bool residual(const Image<const uint16_t> &img,\n                       const Matrix2P &transformed_pattern,\n                       VectorP &residual) const {\n\n    Scalar sum = 0; // hm: stores the sum of image pixels, using valid points in the transformed pattern\n    Vector2 grad_sum(0, 0);\n    int num_valid_points = 0;\n\n    for (int i = 0; i < PATTERN_SIZE; i++) {\n      if (img.InBounds(transformed_pattern.col(i), 2)) {\n        residual[i] = img.interp<Scalar>(transformed_pattern.col(i)); // Yu: bilinear interplatation\n        sum += residual[i];\n        num_valid_points++;\n      } else {\n        residual[i] = -1;\n      }\n    }\n\n    int num_residuals = 0;\n\n    for (int i = 0; i < PATTERN_SIZE; i++) {\n      if (residual[i] >= 0 && data[i] >= 0) {\n        const Scalar val = residual[i];\n        // hm: sum / num_valid_points gives the averaged intensity of the patch\n        residual[i] = num_valid_points * val / sum - data[i];\n        num_residuals++;\n\n      } else {\n        residual[i] = 0;\n      }\n    }\n\n    // hm: num_residuals indicates the number of points that both valid in data and residuals\n    return num_residuals > PATTERN_SIZE / 2;\n  }\n\n  Vector2 pos; //patch center\n  VectorP data;  // template patch, negative if the point is not valid\n  Matrix3P H_se2_inv_J_se2_T; //inv(JT*J)*JT, typical calculation for gaussian newton optimization\n  Scalar mean; //  mean of data, before deviding by mean\n};\n")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"interpolation pixel",src:n(72085).Z}),"\n",(0,r.kt)("img",{alt:"interpolation grad",src:n(39246).Z})),(0,r.kt)("h4",{id:"frame-processing"},"frame processing"),(0,r.kt)("h1",{id:"draft-section"},"Draft section"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},'        "config.optical_flow_detection_grid_size": 50,  // Patch size \n        "config.optical_flow_max_recovered_dist2": 1.0,  //max distance set for optical flow\n        "config.optical_flow_pattern": 51,\n        "config.optical_flow_max_iterations": 5,    \n        "config.optical_flow_epipolar_error": 0.5,       // remove flow that valates the epipolar constrain\n        "config.optical_flow_levels": 5,\n        "config.optical_flow_skip_frames": 1,            // 1 means do not skip\n        "config.feature_match_show": true,               // show detection and tracking result between left and right \n        "config.vio_max_states": 3,\n        "config.vio_max_kfs": 5,\n        "config.vio_min_frames_after_kf": 5,\n        "config.vio_new_kf_keypoints_thresh": 0.7,\n        "config.vio_debug": true,\n        "config.vio_obs_std_dev": 0.5,                    // dev for 3d vision reprojection error\n        "config.vio_obs_huber_thresh": 1.0,               // Another threshod for vision weights\n        "config.vio_min_triangulation_dist": 0.05,        // the translation has to be bigger than square of this value to do trangulation\n        "config.vio_outlier_threshold": 1.0,              // outlier if the norm of the residual is larger than this value\n        "config.vio_filter_iteration": 4,                 // filter outliers in this iter during optimization\n        "config.vio_max_iterations": 7,                   // vio optimization iteration number\n        "config.vio_enforce_realtime": false,\n        "config.vio_use_lm": false,                       // use gaussian newton otherwise\n        "config.vio_lm_lambda_min": 1e-32,\n        "config.vio_lm_lambda_max": 1e2,\n        "config.vio_init_pose_weight": 1e8,                // margH postion and yaw weight\n')),(0,r.kt)("p",null,"pitch; ///Yu: Bytes per unit data. e.g: one row contain pitch * w bytes."),(0,r.kt)("h1",{id:"update-repo"},"Update repo"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"git submodule sync // fix mismatch remote repo issue\ngit submodule update \n")))}m.isMDXComponent=!0},72085:function(e,t,n){"use strict";t.Z=n.p+"assets/images/func_interp-4797eb76f04b329d9536fec09c2347c3.png"},39246:function(e,t,n){"use strict";t.Z=n.p+"assets/images/func_interpgrid-ef14a49d76837ff6f2b6c88a761e40fc.png"},68370:function(e,t,n){"use strict";t.Z=n.p+"assets/images/pyramid-b22d97085573eb9245f7815632c01a08.png"},83002:function(e,t,n){"use strict";t.Z=n.p+"assets/images/the_inverse_compositional_algo-5423aafdb18358b94931aad20f73e7bc.png"}}]);